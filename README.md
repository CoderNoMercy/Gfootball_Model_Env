# Google Research Football 
We choose Google Research Football Environment (Gfootball) as our game environment. Gfootball environment is a novel open-source reinforcement learning environment proposed by Google in 2019. It provides researchers with a highly-optimized game engine that simulates the game of football as well as a set of progressively harder and diverse reinforcement learning scenarios where researchers can test their ideas and algorithms. In our project, we first implemented simple PPO algorithm on several simple football academy scenarios. Experiment results show that PPO algorithm can achieve fairly satisfying performance in some simple scenarios. But its performance degrades as the environment becomes complicated. So we applied multiagent selfplay algorithm to solve scenarios like 11 VS 11.    

## Environment setup
(1) State/observation: Gfootball environment supports 3 different state representations: pixels, super mini-map (SMM) and floats. In our experiment we use the second one as our state representation. Each state consists of four 72\times96 matrices encoding information about the home team, the away team, the ball, and the active player respectively. SMM representation can also be stacked across multiple consecutive time-steps. In our experiment, we stack 4 consecutive SMM states, which generates a 16\times72\times96 tensor in each training step.

(2) Actions and rewards: Gfootball environment supports 20 distinct valid actions and two reward functions. In our experiment, we use full-action set and 'Scoring' reward, which corresponds to the natural reward where each team obtains a +1 reward when scoring a goal, and a -1 reward when conceding one.

## Experiment result: Football Academy scenarios
In our experiment, we trained our PPO agent on 9 distinct football academy scenarios using multi-worker PPO network described in \citep{kurach2019google}. The network architecture we used is a CNN network with 3 conv-relu layers and 1 fc-relu layer. The output for actor is a 20-dimension vector corresponding to the distribution of each valid action. The output for critic is a scalar value corresponding to V_{\theta}(s_{t}) of each state s_{t}. 

To test performance, evaluation stage is separated to 5 groups, 10 episodes in each group for each scenario. After the test, average reward as well as the standard deviation are calculated and compared to the results. Hyper-parameters are presented in the appendix. Training logs and scenario descriptions can be found in the attachments of the project. In the original paper, algorithm performance under different training steps (1M, 5M, 50M) are compared. In our project, we choose 2M steps in training to balance the algorithm performance and training time. From the result we can see that PPO algorithm achieves fairly satisfying results in some of the academy scenarios (in all academy scenarios, the maximum average reward is +1 and the minimum average reward is -1, those scenarios that have no corresponding bar in figure 5 get zero reward).
